"""


"""

from fastapi import APIRouter
from fastapi import Request, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.responses import RedirectResponse, JSONResponse

import asyncio

import json
from pydantic import BaseModel
import time


import requests # <<<--remove if not used!


import ollama # Can remove this in the future

#from Retreival_Pipeline.RAG_Agent import _

# Define the FastAPI Router Object
router = APIRouter(prefix="")

# Define a request model using Pydantic for the chat API
class RagRequest(BaseModel):
    userQuery: str
    conversationHistory: list
    selectedModel: str

# !!! DUMMY ENDPOINT !!!
######################################################################
@router.post("/chat-dummy")
async def handle_rag_request_toy(request: Request):
    """
        - This is a testing dummy endpoint. 
        - It takes user query and conversation history from the frontend, gives it to an LLM (hosted by Ollama) and returns the entire LLM response as a single JSON packet
    """
    try:
        body = await request.json()
        print("Received body:", body)

        rag_request = RagRequest(**body)
        user_query = rag_request.userQuery
        selected_model = "phi3:latest"
        chat_history = rag_request.conversationHistory

        chat_history.append({
            'role': 'user',
            'content': f'USER QUESTION: {user_query}'
        })

        response = ollama.chat(
            model=selected_model,
            messages=chat_history,
            stream=False
        )

        result = response['message']['content']
        return JSONResponse({"response": result})

    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Invalid JSON")
    except Exception as e:
        print(f"Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat-dummy/stream")
async def handle_rag_request_toy_stream(request: Request):
    """
        - This is a testing dummy endpoint
        - It takes user query & conversation history from the frontend and gives it directly to Ollama
        - It streams back tokens as they are generated by the LLM
    """

    body = await request.json()
    rag_request = RagRequest(**body)
    chat_history = rag_request.conversationHistory
    user_query = rag_request.userQuery
    selected_model = rag_request.selectedModel

    chat_history.append({'role': 'user', 'content': user_query})



######################################################################

@router.post("/chat")
async def handle_rag_request(request: Request):
    """ 
        
    
        Args:

        Retruns:

        Raises:
        
    """

    try:
        body = await request.json()
        print("Received body:", body)

        rag_request = RagRequest(**body)

        # Pass to the Orchestrator Agent
        


        user_query = rag_request.userQuery
        selected_model = "phi3:latest"
        chat_history = rag_request.conversationHistory

        chat_history.append({
            'role': 'user',
            'content': f'USER QUESTION: {user_query}'
        })

        response = ollama.chat(
            model=selected_model,
            messages=chat_history,
            stream=False
        )

        result = response['message']['content']
        return JSONResponse({"response": result})

    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Invalid JSON")
    except Exception as e:
        print(f"Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
