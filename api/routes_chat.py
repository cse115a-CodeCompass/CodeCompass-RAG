""" """

import asyncio
import json
import time

import ollama  # Can remove this in the future
import requests  # <<<--remove if not used!
from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import JSONResponse, RedirectResponse, StreamingResponse
from pydantic import BaseModel

from retreival_pipeline.rag_agent import RAG_Agent

from startup.ollama_checks import Ollama_Manager

# Define the FastAPI Router Object
router = APIRouter(prefix="")


# Define a request model using Pydantic for the chat API
class RagRequest(BaseModel):
    userQuery: str
    conversationHistory: list
    selectedModel: str
    user_id: str
    repo_id: str

# !!! DUMMY ENDPOINT !!!
######################################################################
@router.post("/chat-dummy")
async def handle_rag_request_toy(request: Request):
    """
    - This is a testing dummy endpoint.
    - It takes user query and conversation history from the frontend, gives it to an LLM (hosted by Ollama) and returns the entire LLM response as a single JSON packet
    """
    try:
        body = await request.json()
        print("Received body:", body)

        rag_request = RagRequest(**body)
        user_query = rag_request.userQuery
        selected_model = "phi3:latest"
        chat_history = rag_request.conversationHistory

        chat_history.append({"role": "user", "content": f"USER QUESTION: {user_query}"})

        response = ollama.chat(
            model=selected_model, messages=chat_history, stream=False
        )

        result = response["message"]["content"]
        return JSONResponse({"response": result})

    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Invalid JSON")
    except Exception as e:
        print(f"Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat-dummy/stream")
async def handle_rag_request_toy_stream(request: Request):
    """
    - This is a testing dummy endpoint
    - It takes user query & conversation history from the frontend and gives it directly to Ollama
    - It streams back tokens as they are generated by the LLM
    """

    # Parse request body BEFORE creating the streaming response
    try:
        body = await request.json()
        rag_request = RagRequest(**body)
        chat_history = rag_request.conversationHistory
        user_query = rag_request.userQuery
        selected_model = rag_request.selectedModel

    except json.JSONDecodeError as e:
        raise HTTPException(status_code=400, detail="Invalid JSON in request body")
    except KeyError as e:
        raise HTTPException(status_code=400, detail=f"Missing required field: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

    def generate_stream():
        try:
            chat_history.append({"role": "user", "content": user_query})
            stream = ollama.chat(
                model=selected_model, messages=chat_history, stream=True
            )

            for chunk in stream:
                if "message" in chunk and "content" in chunk["message"]:
                    content = chunk["message"]["content"]
                    yield f"data: {json.dumps({'content': content})}\n\n"

            yield "data: [DONE]\n\n"

        except ollama.ResponseError as e:
            error_data = json.dumps({"error": f"Ollama error: {str(e)}"})
            yield f"data: {error_data}\n\n"
        except Exception as e:
            error_data = json.dumps({"error": f"Internal server error: {str(e)}"})
            yield f"data: {error_data}\n\n"

    try:
        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            },
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


######################################################################

@router.get("/chat/available_models")
async def fetch_available_Ollama_Models():
    """

        Args: None
        Returns:
    """
    try:
        ollama_obj = Ollama_Manager()

        available_models = ollama_obj.parse_installed_models()

        return available_models
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Invalid JSON")
    except Exception as e:
        print(f"Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))    

@router.post("/chat/stream")
async def handle_rag_request_stream(request: Request):
    # Parse request body BEFORE creating the streaming response
    try:
        body = await request.json()
        rag_request = RagRequest(**body)
        chat_history = rag_request.conversationHistory
        user_query = rag_request.userQuery
        selected_model = rag_request.selectedModel
        user_id = rag_request.user_id
        repo_id = rag_request.repo_id

    except json.JSONDecodeError as e:
        raise HTTPException(status_code=400, detail="Invalid JSON in request body")
    except KeyError as e:
        raise HTTPException(status_code=400, detail=f"Missing required field: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

    def generate_stream():
        try:
            # Initialize the RAG Agent
            agent_obj = RAG_Agent(user_id, repo_id)

            stream = agent_obj.run(user_query, chat_history, selected_model)

            for chunk in stream:
                if "message" in chunk and "content" in chunk["message"]:
                    content = chunk["message"]["content"]
                    yield f"data: {json.dumps({'content': content})}\n\n"

            yield "data: [DONE]\n\n"

        except ollama.ResponseError as e:
            error_data = json.dumps({"error": f"Ollama error: {str(e)}"})
            yield f"data: {error_data}\n\n"
        except Exception as e:
            error_data = json.dumps({"error": f"Internal server error: {str(e)}"})
            yield f"data: {error_data}\n\n"

    try:
        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            },
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
